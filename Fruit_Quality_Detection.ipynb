{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTZtnbTyRprg"
      },
      "source": [
        "# BACK END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdhr6DcTRuB7"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "juZujSORRyVU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import io\n",
        "from tkinter import Tk, filedialog\n",
        "import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSWuArOaR0IO"
      },
      "source": [
        "Fruit type classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuIjc9oOS4Hj",
        "outputId": "3efb99e5-6630-49f3-fa43-da8667020cdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6272 images belonging to 2 classes.\n",
            "Found 1568 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 211ms/step - accuracy: 0.9241 - loss: 0.2575 - val_accuracy: 0.9573 - val_loss: 0.1026\n",
            "Epoch 2/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - accuracy: 0.9802 - loss: 0.0552 - val_accuracy: 0.9011 - val_loss: 0.2747\n",
            "Epoch 3/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 219ms/step - accuracy: 0.9877 - loss: 0.0363 - val_accuracy: 0.9783 - val_loss: 0.0526\n",
            "Epoch 4/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 212ms/step - accuracy: 0.9938 - loss: 0.0162 - val_accuracy: 0.9770 - val_loss: 0.0632\n",
            "Epoch 5/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 204ms/step - accuracy: 0.9976 - loss: 0.0082 - val_accuracy: 0.9796 - val_loss: 0.0595\n",
            "Epoch 6/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.9947 - loss: 0.0155 - val_accuracy: 0.9649 - val_loss: 0.1220\n",
            "Epoch 7/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - accuracy: 0.9963 - loss: 0.0100 - val_accuracy: 0.9452 - val_loss: 0.1965\n",
            "Epoch 8/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 214ms/step - accuracy: 0.9928 - loss: 0.0233 - val_accuracy: 0.9796 - val_loss: 0.0674\n",
            "Epoch 9/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - accuracy: 0.9974 - loss: 0.0084 - val_accuracy: 0.9815 - val_loss: 0.0440\n",
            "Epoch 10/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 223ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9815 - val_loss: 0.0917\n",
            "Epoch 11/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 4.1850e-04 - val_accuracy: 0.9828 - val_loss: 0.0794\n",
            "Epoch 12/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 219ms/step - accuracy: 1.0000 - loss: 1.5254e-04 - val_accuracy: 0.9847 - val_loss: 0.0788\n",
            "Epoch 13/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 8.3174e-05 - val_accuracy: 0.9841 - val_loss: 0.0777\n",
            "Epoch 14/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 6.8675e-05 - val_accuracy: 0.9841 - val_loss: 0.0816\n",
            "Epoch 15/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 5.9345e-05 - val_accuracy: 0.9841 - val_loss: 0.0842\n",
            "Epoch 16/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 5.0082e-05 - val_accuracy: 0.9847 - val_loss: 0.0736\n",
            "Epoch 17/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 222ms/step - accuracy: 1.0000 - loss: 4.4610e-05 - val_accuracy: 0.9841 - val_loss: 0.0851\n",
            "Epoch 18/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 223ms/step - accuracy: 1.0000 - loss: 3.7826e-05 - val_accuracy: 0.9841 - val_loss: 0.0832\n",
            "Epoch 19/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 3.3829e-05 - val_accuracy: 0.9841 - val_loss: 0.0906\n",
            "Epoch 20/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 1.0000 - loss: 3.0182e-05 - val_accuracy: 0.9841 - val_loss: 0.0877\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    'dataset/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    'dataset/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "fruit_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "fruit_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "fruit_model.fit(train_gen, epochs=20, validation_data=val_gen)\n",
        "\n",
        "fruit_model.save(\"fruit_classifier.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OePqwojS2A0"
      },
      "source": [
        "Quality classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEgeh5XcS7m6",
        "outputId": "c265beb2-510a-4db5-e8ed-40116753ab11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3229 images belonging to 2 classes.\n",
            "Found 806 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 216ms/step - accuracy: 0.8136 - loss: 0.8258 - val_accuracy: 0.8127 - val_loss: 0.4073\n",
            "Epoch 2/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 208ms/step - accuracy: 0.9031 - loss: 0.2368 - val_accuracy: 0.9355 - val_loss: 0.1809\n",
            "Epoch 3/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 214ms/step - accuracy: 0.9263 - loss: 0.1947 - val_accuracy: 0.9181 - val_loss: 0.2064\n",
            "Epoch 4/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 215ms/step - accuracy: 0.9343 - loss: 0.1667 - val_accuracy: 0.9355 - val_loss: 0.1667\n",
            "Epoch 5/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 199ms/step - accuracy: 0.9498 - loss: 0.1270 - val_accuracy: 0.9355 - val_loss: 0.1678\n",
            "Epoch 6/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 205ms/step - accuracy: 0.9554 - loss: 0.1177 - val_accuracy: 0.9467 - val_loss: 0.1307\n",
            "Epoch 7/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 201ms/step - accuracy: 0.9678 - loss: 0.0878 - val_accuracy: 0.9553 - val_loss: 0.1177\n",
            "Epoch 8/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 222ms/step - accuracy: 0.9799 - loss: 0.0595 - val_accuracy: 0.9665 - val_loss: 0.0904\n",
            "Epoch 9/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 213ms/step - accuracy: 0.9793 - loss: 0.0571 - val_accuracy: 0.9653 - val_loss: 0.0903\n",
            "Epoch 10/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 210ms/step - accuracy: 0.9814 - loss: 0.0532 - val_accuracy: 0.9516 - val_loss: 0.1388\n",
            "Epoch 11/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 216ms/step - accuracy: 0.9867 - loss: 0.0415 - val_accuracy: 0.9690 - val_loss: 0.0801\n",
            "Epoch 12/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 204ms/step - accuracy: 0.9923 - loss: 0.0268 - val_accuracy: 0.9194 - val_loss: 0.2181\n",
            "Epoch 13/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 209ms/step - accuracy: 0.9885 - loss: 0.0382 - val_accuracy: 0.9529 - val_loss: 0.1385\n",
            "Epoch 14/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 207ms/step - accuracy: 0.9941 - loss: 0.0211 - val_accuracy: 0.9603 - val_loss: 0.1012\n",
            "Epoch 15/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 212ms/step - accuracy: 0.9935 - loss: 0.0235 - val_accuracy: 0.9690 - val_loss: 0.0711\n",
            "Epoch 16/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 219ms/step - accuracy: 0.9994 - loss: 0.0088 - val_accuracy: 0.9702 - val_loss: 0.1032\n",
            "Epoch 17/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 214ms/step - accuracy: 0.9972 - loss: 0.0158 - val_accuracy: 0.9677 - val_loss: 0.0905\n",
            "Epoch 18/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 207ms/step - accuracy: 0.9988 - loss: 0.0077 - val_accuracy: 0.9640 - val_loss: 0.1192\n",
            "Epoch 19/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 210ms/step - accuracy: 0.9991 - loss: 0.0066 - val_accuracy: 0.9727 - val_loss: 0.0732\n",
            "Epoch 20/20\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9715 - val_loss: 0.0856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3045 images belonging to 2 classes.\n",
            "Found 760 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 235ms/step - accuracy: 0.9002 - loss: 0.6759 - val_accuracy: 0.9855 - val_loss: 0.0479\n",
            "Epoch 2/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 230ms/step - accuracy: 0.9895 - loss: 0.0415 - val_accuracy: 0.9763 - val_loss: 0.0647\n",
            "Epoch 3/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 217ms/step - accuracy: 0.9842 - loss: 0.0422 - val_accuracy: 0.9947 - val_loss: 0.0204\n",
            "Epoch 4/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 226ms/step - accuracy: 0.9967 - loss: 0.0128 - val_accuracy: 0.9987 - val_loss: 0.0084\n",
            "Epoch 5/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 216ms/step - accuracy: 0.9997 - loss: 0.0038 - val_accuracy: 0.9974 - val_loss: 0.0084\n",
            "Epoch 6/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 218ms/step - accuracy: 0.9997 - loss: 0.0026 - val_accuracy: 0.9974 - val_loss: 0.0083\n",
            "Epoch 7/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9987 - val_loss: 0.0071\n",
            "Epoch 8/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 7.3597e-04 - val_accuracy: 0.9987 - val_loss: 0.0058\n",
            "Epoch 9/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 6.4358e-04 - val_accuracy: 0.9987 - val_loss: 0.0068\n",
            "Epoch 10/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 228ms/step - accuracy: 1.0000 - loss: 4.1182e-04 - val_accuracy: 0.9974 - val_loss: 0.0064\n",
            "Epoch 11/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 224ms/step - accuracy: 1.0000 - loss: 2.6938e-04 - val_accuracy: 0.9987 - val_loss: 0.0051\n",
            "Epoch 12/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 1.9607e-04 - val_accuracy: 0.9974 - val_loss: 0.0061\n",
            "Epoch 13/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 1.5936e-04 - val_accuracy: 0.9974 - val_loss: 0.0063\n",
            "Epoch 14/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 1.1451e-04 - val_accuracy: 0.9987 - val_loss: 0.0059\n",
            "Epoch 15/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 1.2368e-04 - val_accuracy: 0.9974 - val_loss: 0.0057\n",
            "Epoch 16/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 8.1546e-05 - val_accuracy: 0.9987 - val_loss: 0.0048\n",
            "Epoch 17/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 6.6813e-05 - val_accuracy: 0.9974 - val_loss: 0.0058\n",
            "Epoch 18/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 236ms/step - accuracy: 1.0000 - loss: 5.4790e-05 - val_accuracy: 0.9987 - val_loss: 0.0052\n",
            "Epoch 19/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 222ms/step - accuracy: 1.0000 - loss: 5.2580e-05 - val_accuracy: 0.9974 - val_loss: 0.0059\n",
            "Epoch 20/20\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 3.8425e-05 - val_accuracy: 0.9974 - val_loss: 0.0066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "apple_train_dir = 'dataset/apple/'\n",
        "banana_train_dir = 'dataset/banana/'\n",
        "\n",
        "apple_gen = datagen.flow_from_directory(\n",
        "    apple_train_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "apple_val_gen = datagen.flow_from_directory(\n",
        "    apple_train_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "apple_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "apple_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "apple_model.fit(apple_gen, epochs=20, validation_data=apple_val_gen)\n",
        "apple_model.save(\"apple_quality_classifier.h5\")\n",
        "\n",
        "banana_gen = datagen.flow_from_directory(\n",
        "    banana_train_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "banana_val_gen = datagen.flow_from_directory(\n",
        "    banana_train_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "banana_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "banana_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "banana_model.fit(banana_gen, epochs=20, validation_data=banana_val_gen)\n",
        "banana_model.save(\"banana_quality_classifier.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the models (If needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Download trained models from Google Drive\n",
        "gdown.download(\"https://drive.google.com/file/d/1jQd_ta74YLn7EDHQ82tt5EhWwFYLa4zT/view?usp=sharing\", \"fruit_classifier.h5\", quiet=False)\n",
        "gdown.download(\"https://drive.google.com/file/d/1z56tVm_9AIy7672ofTTJRYovPqGLtLnq/view?usp=sharing\", \"apple_quality_classifier.h5\", quiet=False)\n",
        "gdown.download(\"https://drive.google.com/file/d/11XTLPyayRPiKz6UmQnOtSJQ72rJXfCJe/view?usp=sharing\", \"banana_quality_classifier.h5\", quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjsbvMAKZ6zh"
      },
      "source": [
        "Evaluating the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluating Fruit Classifier (Apple vs Banana) ===\n",
            "Found 7840 images belonging to 2 classes.\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 88ms/step - accuracy: 0.9968 - loss: 0.0176\n",
            "Fruit Classifier - Validation Loss: 0.0176\n",
            "Fruit Classifier - Validation Accuracy: 0.9968\n",
            "\n",
            "=== Evaluating Apple Quality Classifier (Fresh vs Rotten) ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4035 images belonging to 2 classes.\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 82ms/step - accuracy: 0.9943 - loss: 0.0180\n",
            "Apple Quality Classifier - Validation Loss: 0.0180\n",
            "Apple Quality Classifier - Validation Accuracy: 0.9943\n",
            "\n",
            "=== Evaluating Banana Quality Classifier (Fresh vs Rotten) ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3805 images belonging to 2 classes.\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 93ms/step - accuracy: 0.9995 - loss: 0.0013\n",
            "Banana Quality Classifier - Validation Loss: 0.0013\n",
            "Banana Quality Classifier - Validation Accuracy: 0.9995\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 89ms/step - accuracy: 0.9968 - loss: 0.0176\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.9943 - loss: 0.0180\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 93ms/step - accuracy: 0.9995 - loss: 0.0013\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import Libraries\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize Data Generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# ================================================================\n",
        "print(\"\\n=== Evaluating Fruit Classifier (Apple vs Banana) ===\")\n",
        "\n",
        "# Load model\n",
        "fruit_model = load_model(\"fruit_classifier.h5\")\n",
        "\n",
        "# Prepare validation data (dataset contains 'apple' and 'banana' folders)\n",
        "fruit_generator = datagen.flow_from_directory(\n",
        "    \"dataset\",\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_loss, val_accuracy = fruit_model.evaluate(fruit_generator)\n",
        "print(f\"Fruit Classifier - Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Fruit Classifier - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# ================================================================\n",
        "print(\"\\n=== Evaluating Apple Quality Classifier (Fresh vs Rotten) ===\")\n",
        "\n",
        "# Load model\n",
        "apple_model = load_model(\"apple_quality_classifier.h5\")\n",
        "\n",
        "# Prepare validation data (apple folder contains 'fresh' and 'rotten')\n",
        "apple_generator = datagen.flow_from_directory(\n",
        "    \"dataset/apple\",\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_loss, val_accuracy = apple_model.evaluate(apple_generator)\n",
        "print(f\"Apple Quality Classifier - Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Apple Quality Classifier - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# ================================================================\n",
        "print(\"\\n=== Evaluating Banana Quality Classifier (Fresh vs Rotten) ===\")\n",
        "\n",
        "# Load model\n",
        "banana_model = load_model(\"banana_quality_classifier.h5\")\n",
        "\n",
        "# Prepare validation data (banana folder contains 'fresh' and 'rotten')\n",
        "banana_generator = datagen.flow_from_directory(\n",
        "    \"dataset/banana\",\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_loss, val_accuracy = banana_model.evaluate(banana_generator)\n",
        "print(f\"Banana Quality Classifier - Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Banana Quality Classifier - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# === Visualization of Model Accuracies ===\n",
        "accuracies = [fruit_model.evaluate(fruit_generator)[1],\n",
        "              apple_model.evaluate(apple_generator)[1],\n",
        "              banana_model.evaluate(banana_generator)[1]]\n",
        "models = [\"Fruit Classifier\", \"Apple Quality\", \"Banana Quality\"]\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(models, accuracies, color=[\"#4caf50\", \"#ff9800\", \"#2196f3\"])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.ylim(0, 1)\n",
        "for i, acc in enumerate(accuracies):\n",
        "    plt.text(i, acc + 0.02, f\"{acc:.2f}\", ha='center', fontsize=10)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEj9ceR_TDkq"
      },
      "source": [
        "# FRONT END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "-HaebaLMTRHc",
        "outputId": "4e7c8678-424e-4f5e-81a0-00a94081a014"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "fruit_model = load_model(\"fruit_classifier.h5\")\n",
        "apple_model = load_model(\"apple_quality_classifier.h5\")\n",
        "banana_model = load_model(\"banana_quality_classifier.h5\")\n",
        "\n",
        "def predict_fruit_and_quality(image):\n",
        "    image_resized = image.resize((128, 128))\n",
        "    image_array = img_to_array(image_resized) / 255.0\n",
        "    image_array = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "    fruit_prediction = fruit_model.predict(image_array)[0][0]\n",
        "    if fruit_prediction < 0.5:\n",
        "        fruit = \"Apple\"\n",
        "        quality_model = apple_model\n",
        "    else:\n",
        "        fruit = \"Banana\"\n",
        "        quality_model = banana_model\n",
        "\n",
        "    quality_prediction = quality_model.predict(image_array)[0][0]\n",
        "    quality = \"Fresh\" if quality_prediction < 0.5 else \"Rotten\"\n",
        "\n",
        "    return f\"{fruit}: {quality}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_fruit_and_quality,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Fruit Quality Detection System\",\n",
        "    description=\"Upload an image of an apple or banana to detect the fruit type and check if it's fresh or rotten.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
